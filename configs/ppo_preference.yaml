# PPO training configuration with preference-based reward model
model: EleutherAI/gpt-neo-125M
reward_model_dir: models/reward_model_preference

dataset:
  loader: preference
  name: Anthropic/hh-rlhf
  subset_size: 500  # Moderate size for PPO training
  max_seq_length: 256  # Shorter for faster training
  clean: true
  tokenizer:
    padding: max_length
    truncation: true
    return_tensors: pt

training:
  epochs: 3
  batch_size: 2
  learning_rate: 5e-6  # Lower learning rate for stability
  clip_epsilon: 0.2
  entropy_coeff: 0.01  # Entropy regularization
  logging_steps: 5     # More frequent logging
  save_steps: 25       # More frequent saves
  max_grad_norm: 1.0
  value_coeff: 0.5     # Value function loss coefficient
  gamma: 0.99          # Discount factor
  gae_lambda: 0.95     # GAE lambda parameter

output:
  model_dir: models/ppo_preference_model
  log_dir: logs/ppo_training
