# Reward model training with proper preference data - 7B model
model: microsoft/DialoGPT-medium  # Start with a medium model, upgrade to 7B later

dataset:
  loader: preference
  name: Anthropic/hh-rlhf
  subset_size: 2000  # Increased for better learning
  max_seq_length: 512
  clean: true
  tokenizer:
    padding: max_length
    truncation: true
    return_tensors: pt

training:
  epochs: 5  # More epochs for better convergence
  batch_size: 2  # Smaller batch for memory constraints
  learning_rate: 1e-5  # Lower learning rate for stability
  logging_steps: 25

output:
  model_dir: models/reward_model_7b
