# Test configuration for LoRA training - very small scale
model: EleutherAI/gpt-neo-125M  # Keep small model for testing
use_lora: true
lora_config:
  r: 8  # Smaller LoRA rank for testing
  lora_alpha: 16
  target_modules: ["q_proj", "v_proj"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

dataset:
  loader: preference
  name: Anthropic/hh-rlhf
  subset_size: 20  # Very small for testing
  max_seq_length: 256
  clean: true
  tokenizer:
    padding: max_length
    truncation: true

training:
  epochs: 1  # Single epoch for testing
  batch_size: 1  # Small batch for memory
  learning_rate: 2e-4
  logging_steps: 5
  save_steps: 10
  eval_steps: 10
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 5

output:
  model_dir: models/test_lora_model
  log_dir: logs/test_lora_training
