# Reward model training with proper preference data
model: EleutherAI/gpt-neo-125M  # Will upgrade to larger model in Phase 3

dataset:
  loader: preference
  name: Anthropic/hh-rlhf
  subset_size: 500  # Phase B: Moderate increase for better learning
  max_seq_length: 256  # Shorter for faster training
  clean: true
  tokenizer:
    padding: max_length
    truncation: true

training:
  epochs: 5  # More epochs for better convergence
  batch_size: 4  # Larger batch for stability
  learning_rate: 3e-5  # Slightly lower for stability
  logging_steps: 25  # More frequent logging
  margin: 0.5  # Bradley-Terry margin for better separation

output:
  model_dir: models/reward_model_preference
