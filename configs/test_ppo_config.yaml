# Test configuration for PPO training - very small scale
model: EleutherAI/gpt-neo-125M
reward_model_dir: models/test_reward_model

dataset:
  loader: preference
  name: Anthropic/hh-rlhf
  subset_size: 20  # Very small for testing
  max_seq_length: 128  # Shorter for faster training
  clean: true
  tokenizer:
    padding: max_length
    truncation: true
    return_tensors: pt

training:
  epochs: 1  # Single epoch for testing
  batch_size: 1  # Small batch for memory
  learning_rate: 1e-5
  clip_epsilon: 0.2
  logging_steps: 5
  save_steps: 10
  max_grad_norm: 1.0

output:
  model_dir: models/test_ppo_model
  log_dir: logs/test_ppo_training
